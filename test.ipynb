{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset \n",
    "image_size = 28           # width and length\n",
    "no_of_different_labels = 10     #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "train_data = np.loadtxt(\"mnist_train.csv\", delimiter=\",\")\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", delimiter=\",\") \n",
    "\n",
    "# data preprocessing\n",
    "\n",
    "# map pixels information from range(0, 255) to range(0.01, 1)\n",
    "fac = 0.99 / 255\n",
    "train_imgs = np.asfarray(train_data[:, 1:]) * fac + 0.01\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) * fac + 0.01\n",
    "\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])\n",
    "\n",
    "lr = np.arange(no_of_different_labels)\n",
    "\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(np.float64)\n",
    "test_labels_one_hot = (lr==test_labels).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val=train_test_split(train_imgs,train_labels_one_hot,test_size=0.2,random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, sizes,epochs=100,learning_rate=0.0001):\n",
    "        self.sizes=sizes\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.params=self.initializeWeight()\n",
    "        self.status={}\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "        print(self.params['W1'].shape)\n",
    "    def initializeWeight(self):\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        hidden_2=self.sizes[2]\n",
    "        output_layer=self.sizes[3]\n",
    "        return{\n",
    "            'W1':np.random.randn(input_layer, hidden_1) * np.sqrt(1. / hidden_1),\n",
    "            'W2':np.random.randn(hidden_1, hidden_2) * np.sqrt(1. / hidden_2),\n",
    "            'W3':np.random.randn(hidden_2, output_layer) * np.sqrt(1. / output_layer),\n",
    "            'b1':np.random.randn(hidden_1),\n",
    "            'b2':np.random.randn(hidden_2),\n",
    "            'b3':np.random.randn(output_layer)\n",
    "        }\n",
    "    def InnerProduct_ForProp(self,x,W,b):\n",
    "        y=np.dot(x,W)+b\n",
    "        return y\n",
    "\n",
    "    def InnerProduct_BackProp(self,dEdy,x,W,b):\n",
    "        dEdx=np.dot(dEdy,W.T)\n",
    "        dEdW=np.dot(x.T,dEdy)\n",
    "        dEdb=dEdy\n",
    "        return dEdx,dEdW,dEdb\n",
    "    def Softmax_ForProp(self,x):\n",
    "        exp=np.exp(x)\n",
    "        y=exp/np.sum(exp)\n",
    "        return y\n",
    "\n",
    "    def Softmax_BackProp(self,y,t):\n",
    "        dEdx=y-t\n",
    "        return dEdx\n",
    "\n",
    "    def Sigmoid_ForProp(self,x):\n",
    "        y= 1/(1+np.exp(-x))\n",
    "        return y\n",
    "\n",
    "    def Sigmoid_BackProp(self,dEdy,x):\n",
    "        dEdx= np.exp(-x) / np.square( 1+np.exp(-x) )\n",
    "        return dEdx\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, derivative=False):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        if derivative:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "    def backward_passward(self,output,target):\n",
    "        status=self.status\n",
    "        params=self.params\n",
    "        learning_rate=self.learning_rate\n",
    "        # output layer backpro to hidden 2 layer\n",
    "        soft_back=self.Softmax_BackProp(output,target)\n",
    "        dEdx,dEdW,dEdb=self.InnerProduct_BackProp(soft_back,status[\"A2\"],params[\"W3\"],params[\"b3\"])\n",
    "        # W3 update\n",
    "        w3_grad=np.average(dEdW,axis=0)\n",
    "        b3_grad=np.average(dEdb,axis=0)\n",
    "        self.params[\"W3\"]=self.params[\"W3\"]-w3_grad*learning_rate\n",
    "        self.params[\"b3\"]=self.params[\"b3\"]-b3_grad*learning_rate\n",
    "        \n",
    "        # hidden 2 back to hidden 1\n",
    "        sigm_back=self.Sigmoid_BackProp(dEdx,status[\"Z2\"])\n",
    "        dEdx,dEdW,dEdb=self.InnerProduct_BackProp(sigm_back,status[\"A1\"],params[\"W2\"],params[\"b2\"])\n",
    "\n",
    "\n",
    "        # W2 update\n",
    "        w2_grad=np.average(dEdW,axis=0)\n",
    "        b2_grad=np.average(dEdb,axis=0)\n",
    "        self.params[\"W2\"]-=w2_grad*learning_rate\n",
    "        self.params[\"b2\"]-=b2_grad*learning_rate\n",
    "\n",
    "\n",
    "        # hidden 1 back to input\n",
    "        sigm_back=self.Sigmoid_BackProp(dEdx,status[\"Z1\"])\n",
    "        dEdx,dEdW,dEdb=self.InnerProduct_BackProp(sigm_back,status[\"A0\"],params[\"W1\"],params[\"b1\"])\n",
    "        # W1 update\n",
    "        w1_grad=np.average(dEdW,axis=0)\n",
    "        b1_grad=np.average(dEdb,axis=0)\n",
    "        self.params[\"W1\"]=self.params[\"W1\"]-w1_grad*learning_rate\n",
    "        self.params[\"b1\"]=self.params[\"b1\"]-b1_grad*learning_rate\n",
    "        \n",
    "        return\n",
    "    def forward_passward(self,x):\n",
    "        # input layer to hidden 1 layer\n",
    "        A0=x\n",
    "        Z1=self.InnerProduct_ForProp(A0,self.params['W1'],self.params['b1'])\n",
    "        A1=self.Sigmoid_ForProp(Z1)\n",
    "\n",
    "        # hidden 1 to hidden 2 layer\n",
    "        Z2=self.InnerProduct_ForProp(A1,self.params['W2'],self.params['b2'])\n",
    "        A2=self.Sigmoid_ForProp(Z2)\n",
    "        # hidden 2 to output layer\n",
    "        Z3=self.InnerProduct_ForProp(A2,self.params['W3'],self.params['b3'])\n",
    "        A3=self.Softmax_ForProp(Z3)\n",
    "        self.status={\"A0\":A0,\n",
    "                     \"Z1\":Z1,\n",
    "                     \"A1\":A1,\n",
    "                     \"Z2\":Z2,\n",
    "                     \"A2\":A2,\n",
    "                     \"Z3\":Z3,\n",
    "                     \"A3\":A3}\n",
    "        return A3\n",
    "\n",
    "\n",
    "    def accuracy(self,x,targets):\n",
    "        predictions=[]\n",
    "        outputs=self.forward_pass(x)\n",
    "\n",
    "        for o,t in zip(outputs,targets):\n",
    "            pred=np.argmax(o)\n",
    "            predictions.append(pred==np.argmax(t))\n",
    "\n",
    "        return np.mean(predictions)\n",
    "\n",
    "        pass\n",
    "    def train(self,x_train,y_train,x_val,y_val):\n",
    "        start=time.time()\n",
    "        for i in range(self.epochs):\n",
    "            output=self.forward_pass(x_train)\n",
    "            loss_val=self.loss(torch.tensor(output),torch.tensor(y_train))\n",
    "            \n",
    "            self.backward_pass(output,y_train)\n",
    "            break\n",
    "            print(\"epochs=\"+str(i+1)+\"  Time Spent:\"+str(time.time()-start))\n",
    "            print(self.params['W3'][0])\n",
    "            accuracy=self.accuracy(x_train,y_train)\n",
    "            print(\"accuracy=\",accuracy,\"loss=\",loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nn_model=NeuralNetwork(sizes=[784,128,64,10])\n",
    "nn_model.train(x_train,y_train,x_val,y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
