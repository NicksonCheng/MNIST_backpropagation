# Layer implementation
總共用4層layer
input layer= image size= 28x28=784

hidden layer 1=256

hidden layer 2=128

output layer=10(10個數字)

weight bias一開始用random[0~1] ， 之後透過backpropagation不斷更新參數
## InnerProduct
Y=a*W+b
## Sigmoid
y=1/(1+e^(-x))
## relu
[x>0]*x1
## Softmax layer
y=e^x/sum(e^x)
# Forward / Backward
Forwarding透過innerProduct將神經元傳遞到下一層Layer，再透過sigmoid/ReLu function來判斷是否刺激啟動這個神經元，一層一層傳遞下去，最後產生output layer跟target進行cross entropy比較得出loss，loss再透過backpropagation利用偏微分傳遞回去，然後再利用Gradient Descent概念更新每個神經元的weight以及bias，所有InnerProduct layer以及Activation layer的backpropagation都可以利用偏微分算出公式。
# Accuracy loss plot
validation training accuracy>90%

epochs 40~50趨於平緩即accuracy不再增加


![](https://i.imgur.com/NuyN4s0.png)

# Difficaulty
## Sigmoid function
利用sigmoid function去實作back propagation weight會一直不斷增加，最後在計算過程中會overflow導致輸出nan，推測可能是vanishing gradient，也有可能是我code寫錯，最後換成ReLu function則可以正常運行
## Too many function、variable
使用到的forward backward function以及神經元參數過多不好管理，透過class的方式能夠讓code更好理解